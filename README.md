# Select X Largest Values
## Get Started
Welcome to this project. This documentation will guide you to understand, run and test this project. To have the perfect experiences, please follow this readme step by step. Have fun!
## Prerequisites
To run this project successfully, the following prerequisites must be satisfied:  
1. A laptop with mainstream OS & terminal installed  

| OS Type | Terminal Apps |  
| :-------: | :------------: |  
| Windows | Windows Command Prompt / Windows PowerShell |
| Mac OS X | Terminal / iTerm / Commander One/... |
| Linux | Terminal / Terminator / Guake / ... |

For mine, my laptop OS is **Windows 11**, and I use **Windows Command Prompt** to run this project.  
2. Have Python 3 installed  
* Download & install the latest version of Python 3 from [here](https://www.python.org/downloads/).  
* Run `python --version`.  
If you see it returns a version number like `3.**.*`, that means you have installed python 3 successfully. Otherwise, please configure the python 3 environment correctly before moving forward.  
For mine, my Python version is `3.10.3`.  

## Project Introduction
This project is aimed for selecting top X largest values from a given data file. The format of each row of data would be as the following pattern:  
`<unique record identifier><white_space><numeric value>`  
* `unique record identifer` is a hexadecimal unique id, generated by `uuid.uuid4()`. For more details about uuid, you could visit [here](https://docs.python.org/3/library/uuid.html#uuid.uuid4).  
* `numeric value` is an integer. Given the `int` type in Python 3 can support unlimited size of integer, as long as your laptop memory is sufficient, this field could be any integer.  
* The data file could be extremely large, even bigger than the RAM size of your laptop.

To view how a valid data file looks like, please view the example file `./input_data/sample_data.txt` for more details.

## Data Processing Flow  
This project can perfectly handle extremely large file inputs and outputs. Even when both `sample_data.txt` and X-sized `output.txt` are larger than the RAM size of PC, the program will still work fine, without worrying about running out of memories. How does it achieve this? The flow chart is as follow: 
![Data Processing Flow](./images/Data%20Processing%20Flow.jpg)  

As the chart above suggests:  
1. the large data inputs are splitted into pre-defined fixed size of **data chunks**.  
2. In each data chunk, the data are sorted in descending order.  
3. Then we use a max heap to store all the first elements from each chunk.  
4. Every time we pop the top element from the heap and write to the output file  
5. Add another element to the heap, the newly added element should be from the same data chunk with the previously popped top element at Step 4.  
6. Repeat step 4 & step 5, until we reach X times.  

In a more structural way, the algorithm would be like this:  
```
# Part 1: Scanning data file
while not end of input
    read a row from input to chunk_list
    if len(chunk_list) == chunk_size:
        sort chunk_list in descending order
        write the data in chunk_list to a new chunk_file
        add the first row of chunk_file to the heap
        clear chunk_list
    else:
        add this new row to chunk_list
# Part 2: Output Results
while num_selected < X
    selected = heap.remove()
    num_selected += 1
    write selected to output.txt
    read next number from the selected chunk_file and place on heap
```  

## Complexity Analysis
To calculate the complexity, the following variables are defined:
```
L = The total rows of input data file
X = The number of elements we want to select from the data file
C = The size of each data chunk
L / C = The number of all chunks

```
With the variables above, we could do the complexity analysis:  

1. Time Complexity  
In part 1, we sorted every chunk, this would cost `O(Clog(C))`. Also, there are `L / C` chunks in total. Thus, the TC is:

```
Time Complexity in part 1 = (L / C) * O(Clog(C)) = O((L / C) * Clog(C)) = O(L * log(C))
```
In part 2, we pop the heap for X times, and for each pop, there will be `O(log(L / C))` time consuming for heap adjustment. Therefore, the TC is:
```
Time Complexity in part 2 = O(X * log(L / C))
```
The total TC is:
```
Total Time Complexity = O(L * log(C)) + O(X * log(L / C)) = O(L * log(C) + X * log(L / C))
```
Given that usually `C` would be `1/1000 * L` til `1/10 * L`, `L / C` would be just `10` to `1000`. However, `C` itself could be `1,000,000` or even more. Therefore, usually `L / C` would be significantly smaller than `C`. Also we know that `L >= X`. Therefore, the total TC could be more simplified as:
```
Total Time Complexity = O(L * log(C))
```
2. Space Complexity   
Because we stored data in both memory and disk, the space complexity should be calculated separately.  
* Memory Space Usage  
In part 1, we used a list up to the chunk size to do in-memory sorting, we also used a heap, the heap size is equal to the number of chunks. In part 2, we only used the same heap to output data. Therefore, the total space complexity in memory would be:  
```
Space Complexity in Memory = O(C) + O(L / C)
```
Still, given that usually `L / C` would be significantly smaller than `C`, we have:  
```
Space Complexity in Memory = O(C)
```  
This result demonstrates that how much memory the program uses is only decided by the chunk size. Usually, the chunk size would be around `50MB - 500MB`, which is small enough to be put into the memory of our laptop.  

* Disk Space Usage  
On the other side, all the data chunks are stored on the disk. Therefore, we have:
```
Disk Space Complexity =  L / C * O(C) = O(L / C * C) = O(L)
```
As long as we have a disk large enough to store the `O(L)` sized data file and its chunks, the program will work.

## Run the program
Please follow the steps below to run the program:  
1. Open up the terminal and move to the root directory of this project.  
2. (Optional) If you do not have a data file to use as the input, you could run `generate_sample_data.py` to generate the data file `./input_data/sample_data.txt`. 
    * Modify the parameters passed into `generate_sample_data()` function in the last line of `generate_sample_data.py`
        * **size** - the number of total valid rows in the data file
        * **lower_bound** - the minimum value that those generated numbers could be  
        * **upper_bound** - the maximum value that those generated numbers could be
        * **error_rate** - the ratio of the number of error rows to the number of valid rows. By default the ratio is 0, which means there will be no invalid rows.  
        * **E.g.** - `generate_sample_data(10, -10, 10, 0.1)` would generate 10 valid rows, and approximately an extra `10%` of invalid rows. In each row, the value would be in the range of `[-10, 10]`.  
    * After specifying the parameters, go back to the terminal, and run the following command:  
    ```
    python generate_sample_data.py
    ```  
    This would trigger the `generate_sample_data.py` file to run, and generate the target data file `./input_data/sample_data.txt`. There will be related outputs on the STDOUT console as well.
    * **E.g.** - When I set the parameters as `generate_sample_data(10000000, -10000000, 10000000, 0.01)`, and run the file, the outputs on the console are as follows:  
    ```
    Start to generate sample data. Progress: 0.0%
    Have generated 400000 rows of valid sample data. Progress: 4.0%
    Have generated 800000 rows of valid sample data. Progress: 8.0%
    Have generated 1200000 rows of valid sample data. Progress: 12.0%
    Have generated 1600000 rows of valid sample data. Progress: 16.0%
    Have generated 2000000 rows of valid sample data. Progress: 20.0%
    Have generated 2400000 rows of valid sample data. Progress: 24.0%
    Have generated 2800000 rows of valid sample data. Progress: 28.0%
    Have generated 3200000 rows of valid sample data. Progress: 32.0%
    Have generated 3600000 rows of valid sample data. Progress: 36.0%
    Have generated 4000000 rows of valid sample data. Progress: 40.0%
    Have generated 4400000 rows of valid sample data. Progress: 44.0%
    Have generated 4800000 rows of valid sample data. Progress: 48.0%
    Have generated 5200000 rows of valid sample data. Progress: 52.0%
    Have generated 5600000 rows of valid sample data. Progress: 56.0%
    Have generated 6000000 rows of valid sample data. Progress: 60.0%
    Have generated 6400000 rows of valid sample data. Progress: 64.0%
    Have generated 6800000 rows of valid sample data. Progress: 68.0%
    Have generated 7200000 rows of valid sample data. Progress: 72.0%
    Have generated 7600000 rows of valid sample data. Progress: 76.0%
    Have generated 8000000 rows of valid sample data. Progress: 80.0%
    Have generated 8400000 rows of valid sample data. Progress: 84.0%
    Have generated 8800000 rows of valid sample data. Progress: 88.0%
    Have generated 9200000 rows of valid sample data. Progress: 92.0%
    Have generated 9600000 rows of valid sample data. Progress: 96.0%
    Have generated 10000000 rows of valid sample data. Progress: 100.0%
    Complete generating sample data. Progress: 100.0%

    Total rows: 10100481
    Error rows: 100481
    Valid rows: 10000000
    ```  
    Yours should be similar with mine.  
    Meanwhile, a new file `sample_data.txt` with `10100481` rows under `./input_data/` is generated. 
3. Run the program and select X largest values  
To start the program, you should run the `select_x_values.py` file, and pass in its parameters via command line arguments. More specifically, there are 2 ways to run the file:  
    Specify the absolute (or relative) path of the data file as a command line argument:  
    ```
    python select_x_values.py <X> <Absolute / Relative data file path>
    ```  
    **E.g.**
    ```
    python select_x_values.py 1000000 .\input_data\sample_data.txt
    ```  
    The command above will start the program, select top 1,000,000 largest values from those `10100481` rows, and store their `unique_id` into `output.txt`.  
    You could also directly pass in the content of the data file as **stdin**：  
    ```
    python select_x_values.py <X> < <Absolute / Relative data file path>
    ```
    **E.g.**      
    ```
    python select_x_values.py 1000000 < .\input_data\sample_data.txt    
    ```
    The above command works exactly the same as the previous command which using the file path.  

    After running the command above, besides `output.txt`,there are also some related log outputs on the STDOUT console, which are as follows:  
    ```
    Progress: Have scanned 1009940 rows of data; have created 1 data chunks.
    Progress: Have scanned 2020121 rows of data; have created 2 data chunks.
    Progress: Have scanned 3030283 rows of data; have created 3 data chunks.
    Progress: Have scanned 4040319 rows of data; have created 4 data chunks.
    Progress: Have scanned 5050298 rows of data; have created 5 data chunks.
    Progress: Have scanned 6060256 rows of data; have created 6 data chunks.
    Progress: Have scanned 7070308 rows of data; have created 7 data chunks.
    Progress: Have scanned 8080313 rows of data; have created 8 data chunks.
    Progress: Have scanned 9090486 rows of data; have created 9 data chunks.
    Progress: Have scanned 10100481 rows of data; have created 10 data chunks.

    Data scanning complete.
                Total rows: 10100481
                Total error rows: 100481
                Total valid rows: 10000000
                Total chunks splitted: 10
                Pre-defined chunk size: 1000000

    Start to output data. Progress: 0.0%
    Have output 50000 rows of data. Progress: 5.0%
    Have output 100000 rows of data. Progress: 10.0%
    Have output 150000 rows of data. Progress: 15.0%
    Have output 200000 rows of data. Progress: 20.0%
    Have output 250000 rows of data. Progress: 25.0%
    Have output 300000 rows of data. Progress: 30.0%
    Have output 350000 rows of data. Progress: 35.0%
    Have output 400000 rows of data. Progress: 40.0%
    Have output 450000 rows of data. Progress: 45.0%
    Have output 500000 rows of data. Progress: 50.0%
    Have output 550000 rows of data. Progress: 55.0%
    Have output 600000 rows of data. Progress: 60.0%
    Have output 650000 rows of data. Progress: 65.0%
    Have output 700000 rows of data. Progress: 70.0%
    Have output 750000 rows of data. Progress: 75.0%
    Have output 800000 rows of data. Progress: 80.0%
    Have output 850000 rows of data. Progress: 85.0%
    Have output 900000 rows of data. Progress: 90.0%
    Have output 950000 rows of data. Progress: 95.0%
    Have output 1000000 rows of data. Progress: 100.0%
    Complete outputting data. Progress: 100.0%

    Data processing complete. Total output rows: 1000000. Please check the output.txt file to view the results.

    There are 10 chunk file(s) remained open. Try to close them.
    All chunk files closed. Program ended.
    ```  
    Yours should be similar with mine.  
    As the output logs above suggest, the program is able to scan the whole data file, split it into several data chunk files, and then use the heap to output the results.  
    Noticed that it could validate each row of data, and automatically skip those invalid rows generated in step 2.  
    Also, by default `chunk_size = 1000000`, which means there will be 1 million rows in a chunk. when the input rows are less than 1 million, there will be only 1 chunk. You could customize the `chunk_size` by yourself. The code is located at `line 6 in select_x_values.py`.  
    Moreover, this program would handle all the corner cases successfully as well, including but not limited to the following ones:  
    * When `X >= L`, the program will output all the rows in the data file
    * When `X = 0`, the program will output an empty `output.txt` file  
    * When input data file is empty, the program will output an empty `output.txt` file as well.  
    * When there are no valid data in the input data file, the program will still output an empty `output.txt` file.  
    * When either of the command line argument is invalid, or there is not enough arguments passed in, the program could print an error log and end successfully.

## Test the Project  
### Unit Test
Unit tests are covered for all the functions in both `select_x_values.py` and `generate_sample_data.py`.  
1. Test Files  

| Unit Test Files | Description |  
| :---: | :---: |
| `test_generate_sample_data.py`| Aimed for testing all the functions in `generate_sample_data.py`|  
| `test_select_x_values.py`| Aimed for testing all the functions in `select_x_values.py`|  

2. Test Cases  

* `test_generate_sample_data.py`  

| No. | Target Function | Valid Input? | Description |  
| --- | --- | --- | --- |  
| 1 | `generate_error_field()` | Yes | Test whether the target function could generate an invalid data filed | 
| 2 | `generate_error_data()` | Yes | Test whether the target function could generate an invalid data row |  
| 3 | `generate_sample_data()` | Yes | Test whether the target function could generate a valid data file, and each row is valid |  

* `test_select_x_values.py`  

| No. | Target Function | Valid Input? | Description |  
| --- | --- | --- | --- |  
| 1 | `handle_inputs()` & `processing_data()` | Yes | Test whether the target function could properly handle the inputs from the data file and correctly output the results | 
| 2 | `is_valid_data()` | Yes | Test whether the target function could correctly classify a valid row of data |  
| 3 | `is_valid_data()` | No | Test whether the target function could correctly classify an invalid row of data |   
| 4 | `is_integer()` | Yes | Test whether the target function could correctly classify a valid integer string |  
| 5 | `is_integer()` | No | Test whether the target function could correctly classify an invalid integer string |  
| 6 | `is_hex_uuid()` | Yes | Test whether the target function could correctly classify a valid unique_id |  
| 7 | `is_hex_uuid()` | No | Test whether the target function could correctly classify an invalid unique_id|  
 
* Test Running  
    * Run `test_generate_sample_data.py`  
    Run the following command line your Terminal App:  
    ```
    python test_generate_sample_data.py
    ```  
    You should get the following results:  
    ```
    ..Start to generate sample data. Progress: 0.0%
    Complete generating sample data. Progress: 100.0%

    Total rows: 5
    Error rows: 0
    Valid rows: 5

    .
    ----------------------------------------------------------------------
    Ran 3 tests in 0.010s

    OK

    ```  
    * Run `test_select_x_values.py`  
    Run the following command line your Terminal App:  
    ```
    python test_select_x_values.py
    ```  
    You should get the following results:  
    ```
    Progress: Have scanned 10 rows of data; have created 1 data chunks.

    Data scanning complete.
                Total rows: 10
                Total error rows: 0
                Total valid rows: 10
                Total chunks splitted: 1
                Pre-defined chunk size: 1000000

    Start to output data. Progress: 0.0%
    Complete outputting data. Progress: 100.0%

    Data processing complete. Total output rows: 3. Please check the output.txt file to view the results.

    There are 1 chunk file(s) remained open. Try to close them.
    All chunk files closed. Program ended.
    ......
    ----------------------------------------------------------------------
    Ran 6 tests in 0.061s

    OK

    ```  
### Manual Test  
To further test the reliability of the program, the following manual tests were conducted:  
* Test `generate_sample_data.py`  

| No. | Inputs | Output as Expected? |  
| --- | --- | --- |  
| 1 | `generate_sample_data(10, -10, 10, 0.2)` | Yes |  
| 2 | `generate_sample_data(1, 0, 1, 0.0)` | Yes |  
| 3 | `generate_sample_data(0, -1, 1, 0.0)` | Yes |  
| 4 | `generate_sample_data(100, 1, 1, 0.0)` | Yes |  
| 5 | `generate_sample_data(100, -10, 10, 1.0)` | Yes |   
| 6 | `generate_sample_data(10000000, -10000000, 10000000, 0.01)` | Yes |  
| 7 | `generate_sample_data(50000000, -100000000, 100000000, 0.2)` | Yes |  

* Test `select_x_values.py`  

| No. | X | Data File | Chunk Size | Output as Expected? |  
| --- | --- | --- | --- | --- |  
| 1 | 1 | 100 valid rows | 1000 | Yes |  
| 2 | 5 | 10 valid rows | 2 | Yes |  
| 3 | 11 | 10 valid rows | 2 | Yes |  
| 4 | 20 | 100 valid rows | 1 | Yes |  
| 5 | 0 | 100 valid rows | 10 | Yes |    
| 6 | 10 | 0 rows | 10 | Yes |  
| 7 | 10 | 0 valid rows + 10 error rows | 10 | Yes |  
| 8 | N / A | N / A | 100 | Yes |
| 9 | 10 | N / A | 100 | Yes |  
| 10 | N / A | 10 rows | 100 | Yes |  
| 11 | 1000000 | 10000000 valid rows + 100481 error rows | 1000000 | Yes |  
| 12 | 10 | 10000000 valid rows + 300256 error rows | 2000000 | Yes |  
| 13 | 50000001 | 50000000 valid rows | 5000000 | Yes |  
| 14 | 1 | 30000000 valid rows | 600000 | Yes |  
| 15 | 8000000 | 100000000 valid rows + 50000000 error rows | 5000000 | Yes |  

You've reached the end of this doc, wish you could have fun!